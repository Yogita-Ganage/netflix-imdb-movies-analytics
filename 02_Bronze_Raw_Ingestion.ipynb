{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3ac1b75-8d15-4b11-a260-9f380e4a5154",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Bronze Layer – Ingestion (Assumptions & Design Notes)\n",
    "**Purpose:**  \n",
    "The Bronze layer stores raw data exactly as received, with no business rules applied.  \n",
    "All datasets are ingested from the workspace Volume, converted to Delta format, and enriched with basic metadata for lineage.\n",
    "\n",
    "**Key Assumptions**\n",
    "- IMDB data is available year-wise in separate folders (e.g. /imdb/2010/).  \n",
    "- Only IMDB files from 2010–2025 are required, based on the task specification.  \n",
    "  Earlier years are ignored intentionally.\n",
    "- Column names are sanitized to be Unity Catalog-safe (no spaces, special characters).\n",
    "- Metadata added in Bronze includes:\n",
    "  - ingestionDate\n",
    "  - source\n",
    "  - fileName\n",
    "  - fileYear (for IMDB datasets only)\n",
    "\n",
    "**Output**\n",
    "Each Bronze table serves as the raw foundation for Silver transformations.\n",
    "Bronze produces five tables:\n",
    "- netflix_titles_raw\n",
    "- netflix_credits_raw\n",
    "- imdb_movies_raw\n",
    "- imdb_advanced_movies_details_raw\n",
    "- imdb_merged_movies_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3eba3e08-b7ca-48c9-98f4-53f8c35c63f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "##Catalog and Schema Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a0ebb2a-f504-46d4-9d5b-09242fe90ecb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG workspace;\n",
    "CREATE SCHEMA IF NOT EXISTS bronze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be8f3ca0-b71c-47e4-bd3f-b04f2849f972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee3e6d8-dec6-45dc-8fbe-2b17110f49b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "514f5968-5772-4c0a-9810-8c46f24f92dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f094fbae-c10d-40a9-8bd7-2de3090d6719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# consistent timestamps\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "# Raw Paths (Volumes)\n",
    "\n",
    "BASE_RAW_PATH    = \"/Volumes/workspace/landing/inbox/source_v1\"\n",
    "NETFLIX_RAW_PATH = f\"{BASE_RAW_PATH}/netflix\"\n",
    "IMDB_RAW_PATH    = f\"{BASE_RAW_PATH}/imdb\"\n",
    "\n",
    "# Netflix CSV paths\n",
    "NETFLIX_TITLES_PATH  = f\"{NETFLIX_RAW_PATH}/titles.csv\"\n",
    "NETFLIX_CREDITS_PATH = f\"{NETFLIX_RAW_PATH}/credits.csv\"\n",
    "\n",
    "# IMDB years as per acceptance criteria (2010–2025)\n",
    "IMDB_YEARS = list(range(2010, 2026))\n",
    "\n",
    "\n",
    "\n",
    "# Helper: clean column names for Unity Catalog\n",
    "\n",
    "def clean_column_names(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Make column names Unity Catalog–safe:\n",
    "    - strip spaces + hidden chars\n",
    "    - replace ANY non [A-Za-z0-9_] with '_'\n",
    "    - collapse multiple '_' into one\n",
    "    - if name starts with digit, prefix with 'c_'\n",
    "    \"\"\"\n",
    "\n",
    "    new_cols = []\n",
    "    for col in df.columns:\n",
    "        col_clean = col.strip()\n",
    "        col_clean = re.sub(r\"[\\t\\n\\r]\", \"\", col_clean)        \n",
    "        col_clean = re.sub(r\"[^A-Za-z0-9_]\", \"_\", col_clean)  \n",
    "        col_clean = re.sub(r\"_+\", \"_\", col_clean)            \n",
    "        if re.match(r\"^[0-9]\", col_clean):                    \n",
    "            col_clean = \"c_\" + col_clean\n",
    "        if col_clean == \"\":\n",
    "            col_clean = \"col_unnamed\"\n",
    "        new_cols.append(col_clean)\n",
    "\n",
    "    for old, new in zip(df.columns, new_cols):\n",
    "        if old != new:\n",
    "            df = df.withColumnRenamed(old, new)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Generic helper for Netflix Bronze tables\n",
    "\n",
    "def create_bronze_table(\n",
    "    csv_path: str,\n",
    "    table_name: str,\n",
    "    source_name: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Read a raw CSV and create a Bronze Delta table in workspace.bronze.\n",
    "\n",
    "    - Keeps data as-is (no business transforms)\n",
    "    - Cleans column names for UC\n",
    "    - Adds ingestion metadata columns\n",
    "    \"\"\"\n",
    "\n",
    "    raw_df: DataFrame = (\n",
    "        spark.read\n",
    "             .option(\"header\", \"true\")\n",
    "             .option(\"quote\", \"\\\"\")       \n",
    "             .option(\"escape\", \"\\\"\")\n",
    "             .option(\"multiLine\", \"true\") \n",
    "             .option(\"mode\", \"PERMISSIVE\")\n",
    "             .csv(csv_path)\n",
    "    )\n",
    "\n",
    "    raw_df = clean_column_names(raw_df)\n",
    "\n",
    "    bronze_df = (\n",
    "        raw_df\n",
    "        .withColumn(\"ingestionDate\", F.current_timestamp())\n",
    "        .withColumn(\"source\", F.lit(source_name))\n",
    "        .withColumn(\"fileName\", F.lit(csv_path))  \n",
    "    )\n",
    "\n",
    "    # For small–medium CSVs: avoid many tiny files\n",
    "    bronze_df = bronze_df.repartition(1)\n",
    "\n",
    "    (\n",
    "        bronze_df.write\n",
    "                 .format(\"delta\")\n",
    "                 .mode(\"overwrite\")\n",
    "                 .option(\"overwriteSchema\", \"true\")\n",
    "                 .saveAsTable(f\"workspace.bronze.{table_name}\")\n",
    "    )\n",
    "\n",
    "    print(f\"Created Bronze table workspace.bronze.{table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cb4773b-c27d-4c07-ae59-750506f9f134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##NETFLIX + IMDB Bronze tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a4ce7be-ef7d-4bcd-aa48-440ef8047ed5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Netflix Bronze tables\n",
    "\n",
    "create_bronze_table(\n",
    "    csv_path=NETFLIX_TITLES_PATH,\n",
    "    table_name=\"netflix_titles_raw\",\n",
    "    source_name=\"netflix_titles_csv\"\n",
    ")\n",
    "\n",
    "create_bronze_table(\n",
    "    csv_path=NETFLIX_CREDITS_PATH,\n",
    "    table_name=\"netflix_credits_raw\",\n",
    "    source_name=\"netflix_credits_csv\"\n",
    ")\n",
    "\n",
    "\n",
    "# IMDB Bronze helper: loop over years\n",
    "\n",
    "def build_imdb_bronze_table(\n",
    "    file_name_pattern: str,\n",
    "    table_name: str,\n",
    "    source_name: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Read IMDB CSVs per year (2010–2025), clean column names,\n",
    "    add metadata, union them and write to a single Bronze Delta table.\n",
    "\n",
    "    file_name_pattern examples (relative to each year folder):\n",
    "        \"imdb_movies_{year}.csv\"\n",
    "        \"advanced_movies_details_{year}.csv\"\n",
    "        \"merged_movies_data_{year}.csv\"\n",
    "    \"\"\"\n",
    "\n",
    "    combined_df = None\n",
    "\n",
    "    for year in IMDB_YEARS:\n",
    "        path = f\"{IMDB_RAW_PATH}/{year}/\" + file_name_pattern.format(year=year)\n",
    "        print(f\"Reading: {path}\")\n",
    "\n",
    "        df_year = (\n",
    "            spark.read\n",
    "                 .option(\"header\", \"true\")\n",
    "                 .option(\"quote\", \"\\\"\")\n",
    "                 .option(\"escape\", \"\\\"\")\n",
    "                 .option(\"multiLine\", \"true\")\n",
    "                 .option(\"mode\", \"PERMISSIVE\")\n",
    "                 .csv(path)\n",
    "        )\n",
    "\n",
    "        df_year = clean_column_names(df_year)\n",
    "\n",
    "        df_year = (\n",
    "            df_year\n",
    "            .withColumn(\"fileYear\", F.lit(year))\n",
    "            .withColumn(\"ingestionDate\", F.current_timestamp())\n",
    "            .withColumn(\"source\", F.lit(source_name))\n",
    "            .withColumn(\"fileName\", F.lit(path)) \n",
    "        )\n",
    "\n",
    "        combined_df = (\n",
    "            df_year\n",
    "            if combined_df is None\n",
    "            else combined_df.unionByName(df_year, allowMissingColumns=True)\n",
    "        )\n",
    "\n",
    "    # Reduce small files\n",
    "    combined_df = combined_df.repartition(4)\n",
    "\n",
    "    (\n",
    "        combined_df.write\n",
    "                   .format(\"delta\")\n",
    "                   .mode(\"overwrite\")\n",
    "                   .option(\"overwriteSchema\", \"true\")\n",
    "                   .saveAsTable(f\"workspace.bronze.{table_name}\")\n",
    "    )\n",
    "\n",
    "    print(f\"Created Bronze table workspace.bronze.{table_name}\")\n",
    "\n",
    "\n",
    "# IMDB Bronze tables (3 unified tables)\n",
    "\n",
    "build_imdb_bronze_table(\n",
    "    file_name_pattern=\"imdb_movies_{year}.csv\",\n",
    "    table_name=\"imdb_movies_raw\",\n",
    "    source_name=\"imdb_movies_csv\"\n",
    ")\n",
    "\n",
    "build_imdb_bronze_table(\n",
    "    file_name_pattern=\"advanced_movies_details_{year}.csv\",\n",
    "    table_name=\"imdb_advanced_movies_details_raw\",\n",
    "    source_name=\"imdb_advanced_movies_details_csv\"\n",
    ")\n",
    "\n",
    "build_imdb_bronze_table(\n",
    "    file_name_pattern=\"merged_movies_data_{year}.csv\",\n",
    "    table_name=\"imdb_merged_movies_raw\",\n",
    "    source_name=\"imdb_merged_movies_data_csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db4d4a86-b26b-4c6a-a4e1-1cb52d6ada9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deec5256-fc9e-4b56-b710-218b929fcc3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG workspace;\n",
    "\n",
    "SELECT 'netflix_titles_raw' AS table_name, COUNT(*) AS row_count\n",
    "FROM bronze.netflix_titles_raw\n",
    "UNION ALL\n",
    "SELECT 'netflix_credits_raw', COUNT(*) FROM bronze.netflix_credits_raw\n",
    "UNION ALL\n",
    "SELECT 'imdb_movies_raw', COUNT(*) FROM bronze.imdb_movies_raw\n",
    "UNION ALL\n",
    "SELECT 'imdb_advanced_movies_details_raw', COUNT(*) FROM bronze.imdb_advanced_movies_details_raw\n",
    "UNION ALL\n",
    "SELECT 'imdb_merged_movies_raw', COUNT(*) FROM bronze.imdb_merged_movies_raw;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6285f951-9285-4eb6-ad37-8bea88cea7c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Netflix Row Count Validation\n",
    "\n",
    "\n",
    "# RAW row counts (direct CSV read, same options as Bronze)\n",
    "raw_titles_count = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .option(\"quote\", \"\\\"\")\n",
    "         .option(\"escape\", \"\\\"\")\n",
    "         .option(\"multiLine\", True)\n",
    "         .option(\"mode\", \"PERMISSIVE\")\n",
    "         .csv(f\"{NETFLIX_RAW_PATH}/titles.csv\")\n",
    "         .count()\n",
    ")\n",
    "\n",
    "raw_credits_count = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .option(\"quote\", \"\\\"\")\n",
    "         .option(\"escape\", \"\\\"\")\n",
    "         .option(\"multiLine\", True)\n",
    "         .option(\"mode\", \"PERMISSIVE\")\n",
    "         .csv(f\"{NETFLIX_RAW_PATH}/credits.csv\")\n",
    "         .count()\n",
    ")\n",
    "\n",
    "# BRONZE row counts (Delta tables)\n",
    "bronze_titles_count  = spark.table(\"workspace.bronze.netflix_titles_raw\").count()\n",
    "bronze_credits_count = spark.table(\"workspace.bronze.netflix_credits_raw\").count()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "863c6e02-554b-48dc-ad3e-c4217f8f4862",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Print summary\n",
    "\n",
    "\n",
    "print(\"RAW Titles rows:     \", raw_titles_count)\n",
    "print(\"BRONZE Titles rows:  \", bronze_titles_count)\n",
    "print(\"Titles removed:      \", raw_titles_count - bronze_titles_count)\n",
    "\n",
    "print(\"\\nRAW Credits rows:    \", raw_credits_count)\n",
    "print(\"BRONZE Credits rows: \", bronze_credits_count)\n",
    "print(\"Credits removed:     \", raw_credits_count - bronze_credits_count)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6399644130178994,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Bronze_Raw_Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
