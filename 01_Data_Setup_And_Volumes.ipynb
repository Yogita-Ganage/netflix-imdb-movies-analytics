{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c8f3628-1995-4794-9546-1ce9c2aebc8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Data Setup\n",
    "\n",
    "The original dataset for this assignment was sourced from Kaggle and uploaded as a ZIP file in the GitHub release as an artifact. For this assignment, we treat the GitHub release artifact link as an external data source.\n",
    "\n",
    "> **Note:** In real-world scenarios, data may be ingested from external sources such as Amazon S3, Azure Blob Storage, FTP servers, or other cloud storage solutions. For simplicity and to avoid manual uploads, we assume the GitHub release artifact link serves as our external source in this assignment.\n",
    "\n",
    "The data remains in its original format and is accessed directly from the provided artifact link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ef437ac-a720-48c5-ad54-ae64bd3fb6ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG workspace;\n",
    "\n",
    "-- 1. Create a Schema specifically for file ingestion (distinct from 'bronze' tables)\n",
    "CREATE SCHEMA IF NOT EXISTS landing;\n",
    "\n",
    "-- 2. Create a Volume named 'inbox'\n",
    "CREATE VOLUME IF NOT EXISTS landing.inbox;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e7991b1-5291-49c0-b00e-61e6d202c6f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "- **Fetched Data from External Source:**  \n",
    "  We downloaded a ZIP file containing raw datasets directly from a GitHub release artifact link, simulating real-world external data ingestion.\n",
    "\n",
    "- **Parallel Data Upload:**  \n",
    "  To speed up the extraction and upload process, we used parallel processing. By leveraging multiple CPU cores, each file in the ZIP archive was extracted simultaneously using threads.\n",
    "\n",
    "- **Why Parallel Extraction?**  \n",
    "  - **Faster Uploads:** Multiple files are processed at the same time, significantly reducing total upload time compared to sequential extraction.\n",
    "  - **Efficient Resource Utilization:** The number of threads is dynamically set based on available CPU cores, ensuring optimal performance.\n",
    "  - **Scalable:** This approach is especially beneficial for large datasets or archives with many files.\n",
    "\n",
    "- **Result:**  \n",
    "  The data is quickly and efficiently uploaded to the target volume, ready for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "772f6227-8301-4fab-96b9-fe4872a8930e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os \n",
    "\n",
    "VOLUME_ROOT = \"/Volumes/workspace/landing/inbox\"\n",
    "TARGET_FOLDER_NAME = \"source_v1\" \n",
    "GITHUB_ZIP_URL = \"https://github.com/Yogita-Ganage/netflix-imdb-movies-analytics/releases/download/v1/raw_files.zip\"\n",
    "\n",
    "FINAL_DESTINATION = f\"{VOLUME_ROOT}/{TARGET_FOLDER_NAME}\"\n",
    "ZIP_PATH = f\"{VOLUME_ROOT}/download.zip\"\n",
    "\n",
    "# MAX_THREADS based on available CPU cores\n",
    "MAX_THREADS = min(32, max(4, multiprocessing.cpu_count()))\n",
    "\n",
    "def extract_file_worker(zip_path, file_info, target_root):\n",
    "    \"\"\"\n",
    "    Extracts a single file from the zip archive to the target directory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        target_path = os.path.join(target_root, file_info.filename)\n",
    "        os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "            with zf.open(file_info) as source, open(target_path, \"wb\") as target:\n",
    "                shutil.copyfileobj(source, target)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting {file_info.filename}: {e}\")\n",
    "        return False\n",
    "\n",
    "def bootstrap_threaded():\n",
    "    if os.path.exists(f\"{FINAL_DESTINATION}/netflix/titles.csv\"):\n",
    "        print(f\"Valid data exists at {FINAL_DESTINATION}. Skipping.\")\n",
    "        print_paths()\n",
    "        return\n",
    "\n",
    "    print(f\"Initializing Parallel Ingestion (Batch Size: {MAX_THREADS})...\")\n",
    "    \n",
    "    if os.path.exists(FINAL_DESTINATION):\n",
    "        shutil.rmtree(FINAL_DESTINATION)\n",
    "\n",
    "    try:\n",
    "        print(\"   1. Downloading stream to Volume...\")\n",
    "        urllib.request.urlretrieve(GITHUB_ZIP_URL, ZIP_PATH)\n",
    "        \n",
    "        print(\"   2. Analyzing Zip structure...\")\n",
    "        with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "            all_files = zip_ref.infolist()\n",
    "            files_to_extract = [f for f in all_files if not f.is_dir()]\n",
    "            temp_extract_path = f\"{VOLUME_ROOT}/_temp_parallel\"\n",
    "            \n",
    "        print(f\"   3. Extracting {len(files_to_extract)} files using {MAX_THREADS} threads...\")\n",
    "        with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "            futures = [\n",
    "                executor.submit(extract_file_worker, ZIP_PATH, f, temp_extract_path) \n",
    "                for f in files_to_extract\n",
    "            ]\n",
    "            for future in futures:\n",
    "                future.result()\n",
    "\n",
    "        print(\"   4. Finalizing structure...\")\n",
    "        items = os.listdir(temp_extract_path)\n",
    "        if not items:\n",
    "            raise Exception(\"Extraction failed.\")\n",
    "        extracted_folder = items[0]\n",
    "        shutil.move(f\"{temp_extract_path}/{extracted_folder}\", FINAL_DESTINATION)\n",
    "        shutil.rmtree(temp_extract_path)\n",
    "\n",
    "        print(\"Success.\")\n",
    "        print_paths()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        if os.path.exists(FINAL_DESTINATION):\n",
    "            shutil.rmtree(FINAL_DESTINATION)\n",
    "        raise e\n",
    "    finally:\n",
    "        if os.path.exists(ZIP_PATH):\n",
    "            os.remove(ZIP_PATH)\n",
    "\n",
    "def print_paths():\n",
    "    print(f\"{FINAL_DESTINATION}/netflix\")\n",
    "    print(f\"{FINAL_DESTINATION}/imdb\")\n",
    "\n",
    "bootstrap_threaded()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6399644130178966,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Data_Setup_And_Volumes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
