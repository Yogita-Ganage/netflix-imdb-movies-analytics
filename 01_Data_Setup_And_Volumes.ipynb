{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ef437ac-a720-48c5-ad54-ae64bd3fb6ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG workspace;\n",
    "\n",
    "-- 1. Create a Schema specifically for file ingestion (distinct from 'bronze' tables)\n",
    "CREATE SCHEMA IF NOT EXISTS landing;\n",
    "\n",
    "-- 2. Create a Volume named 'inbox'\n",
    "CREATE VOLUME IF NOT EXISTS landing.inbox;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "772f6227-8301-4fab-96b9-fe4872a8930e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os \n",
    "\n",
    "VOLUME_ROOT = \"/Volumes/workspace/landing/inbox\"\n",
    "TARGET_FOLDER_NAME = \"source_v1\" \n",
    "GITHUB_ZIP_URL = \"https://github.com/Yogita-Ganage/netflix-imdb-movies-analytics/releases/download/v1/raw_files.zip\"\n",
    "\n",
    "FINAL_DESTINATION = f\"{VOLUME_ROOT}/{TARGET_FOLDER_NAME}\"\n",
    "ZIP_PATH = f\"{VOLUME_ROOT}/download.zip\"\n",
    "\n",
    "# MAX_THREADS based on available CPU cores\n",
    "MAX_THREADS = min(32, max(4, multiprocessing.cpu_count()))\n",
    "\n",
    "def extract_file_worker(zip_path, file_info, target_root):\n",
    "    \"\"\"\n",
    "    Extracts a single file from the zip archive to the target directory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        target_path = os.path.join(target_root, file_info.filename)\n",
    "        os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "            with zf.open(file_info) as source, open(target_path, \"wb\") as target:\n",
    "                shutil.copyfileobj(source, target)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting {file_info.filename}: {e}\")\n",
    "        return False\n",
    "\n",
    "def bootstrap_threaded():\n",
    "    if os.path.exists(f\"{FINAL_DESTINATION}/netflix/titles.csv\"):\n",
    "        print(f\"Valid data exists at {FINAL_DESTINATION}. Skipping.\")\n",
    "        print_paths()\n",
    "        return\n",
    "\n",
    "    print(f\"Initializing Parallel Ingestion (Batch Size: {MAX_THREADS})...\")\n",
    "    \n",
    "    if os.path.exists(FINAL_DESTINATION):\n",
    "        shutil.rmtree(FINAL_DESTINATION)\n",
    "\n",
    "    try:\n",
    "        print(\"   1. Downloading stream to Volume...\")\n",
    "        urllib.request.urlretrieve(GITHUB_ZIP_URL, ZIP_PATH)\n",
    "        \n",
    "        print(\"   2. Analyzing Zip structure...\")\n",
    "        with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "            all_files = zip_ref.infolist()\n",
    "            files_to_extract = [f for f in all_files if not f.is_dir()]\n",
    "            temp_extract_path = f\"{VOLUME_ROOT}/_temp_parallel\"\n",
    "            \n",
    "        print(f\"   3. Extracting {len(files_to_extract)} files using {MAX_THREADS} threads...\")\n",
    "        with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "            futures = [\n",
    "                executor.submit(extract_file_worker, ZIP_PATH, f, temp_extract_path) \n",
    "                for f in files_to_extract\n",
    "            ]\n",
    "            for future in futures:\n",
    "                future.result()\n",
    "\n",
    "        print(\"   4. Finalizing structure...\")\n",
    "        items = os.listdir(temp_extract_path)\n",
    "        if not items:\n",
    "            raise Exception(\"Extraction failed.\")\n",
    "        extracted_folder = items[0]\n",
    "        shutil.move(f\"{temp_extract_path}/{extracted_folder}\", FINAL_DESTINATION)\n",
    "        shutil.rmtree(temp_extract_path)\n",
    "\n",
    "        print(\"Success.\")\n",
    "        print_paths()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        if os.path.exists(FINAL_DESTINATION):\n",
    "            shutil.rmtree(FINAL_DESTINATION)\n",
    "        raise e\n",
    "    finally:\n",
    "        if os.path.exists(ZIP_PATH):\n",
    "            os.remove(ZIP_PATH)\n",
    "\n",
    "def print_paths():\n",
    "    print(f\"{FINAL_DESTINATION}/netflix\")\n",
    "    print(f\"{FINAL_DESTINATION}/imdb\")\n",
    "\n",
    "bootstrap_threaded()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6399644130178966,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Data_Setup_And_Volumes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
